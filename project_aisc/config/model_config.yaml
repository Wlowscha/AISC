input shape: &input_shape 33
latent dim: &latent_dim 300

phi setup:
  input: *input_shape
  layers:
    - {'units': 300,'activation':'relu'}
    - {'units': 300,'activation':'relu'}
    - {'units': 300,'activation':'relu'}
  output: {'units': *latent_dim, 'activation':'linear', 'activity_regularizer':'l1'}

regressor rho setup:
  input: *latent_dim
  layers:
    - {'units': 300,'activation':'relu'}
    - {'units': 300,'activation':'relu'}
    - {'units': 300,'activation':'relu'}
  output: {'units': 1, 'activation':'relu', 'activity_regularizer':'l1'}
